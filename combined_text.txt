Transcription:
In the next 17 minutes, I will give you an overview of the most important machine learning algorithms to help you decide which one is right for your problem. My name is Tim and I have been a data scientist for over 10 years and taught all of these algorithms to hundreds of students in real-life machine learning boot camps. There's a simple strategy for picking the right algorithm for your problem. In 17 minutes, you will know how to pick the right one for any problem and get a basic intuition of each algorithm and how they relate to each other. My goal is to give as many of you as possible an intuitive understanding of the major machine learning algorithms to make you stop feeling overwhelmed. According to Wikipedia, machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Much of the recent advancements in AI are driven by neural networks, which I hope to give you an intuitive understanding of by the end of this video. Let's divide machine learning into its subfields. Generally machine learning is divided into two areas, supervised learning and unsupervised learning.

OCR Text from Slides:
scikitlearn algorithm cheat-sheet   DATA SCIENTIST  OVER 10 YEARS    YOU WILL KNOW  HOW TO PICK THE RIGHT ONE  MY GOAL IS TO GIVE  MAJOR MACHINE LEARNING ALGORITHMS  mn  Machine learning From Wpeda. te te enycpeds  Forth our, s00 Machin Learning Gouna,  Bq saanguages ¥  ead Eat Vewniiony Teas ¥  “Statistical earning” redrocts ore. Fr slastcal leaning in gusts, 00 statist larring in languoge acquston.  Mii ering (ML) of tI NARA «concord wh the ‘evelopment and study of stastical ln that can lean rom data end generalize to unsoen data and thus porto tasks without expt instructions" Recent, artifical neural networks have Boon alo to surpass many previous approaches in  perormance 2!  [ML nds application in many elds inclu natural anguage processing, computer sion, speech recognition, ema fiteing, arcu, and medicine 14 When applied to businoss probs itis known under the name precicwve analytes. though not all ‘mactine taring fs statiscaly based, computabonal statistics fs an important source of the elss methods  “The mathematica! oundatons of Ml. are provide by mathematical optimzaton (matnomatcal programming) methods. Data mining sa related (paral) flo study, focusing on exploratory data anayss (EDA) trough unsuporsed loaning 171  From a theoretical wewponnt, probably approxmatlycorect (PAC) learning powdes &  Co] ot ot  ehustering  ‘imensionality eduction ‘ [0  Co  ot  0  Machine learning and data mining aegens  Probleme BH   Machine learning By ssanguages ¥ ewe Tak eat Eat Vewnsony Tos ¥ From Wipe he ree enyoopeda  For the jour, s00 Machine Learning Gouna, “Satisical earning” redrocts hore. Fr slastcal leaning in gusts, 00 statist larring in anguoge aequston  "Machine earning (ML) ldo study natal inoigence concomod wih the Tae ‘Machine learni to unsoen data and thus prfor tasks witout expbetinstuctons Recent artical ran aka AM neural networks have Boon alo to surpass many previous approaches in en perormanco —. ML nds appticaton n many feds includng natural language processing, computor ‘Supervised earring sion, speech recognition, emai fiteing, arcu, and medicine 4 When applied a) to businoss probs itis known under the name predicve analytes. though not all Clustering ‘machine Yaring ts statistical based, computatonal statistics isan important soutco of | /Binenslonsiy reduction the olss methods =  ‘The mathematica! oundatons of Mare provide by mathomatcal opimizaton (ematnomatcal programming) methods Datamining sa relaed (paral) lof stay, focusing on exploratory data anayss (EDA) trough unsupervised loaning 11  TEE Ea  From a theoretical wewpont, probably approxmatlycortect (PAC) learning prowdes &  19805-ERA NEURAL NETWORK DEEP LEARNING NEURAL NETWORK  Hidden Multiple hid i  Iden layers process ie  ical features  Links cary signals from one node: +0 another, boosting ‘or damping them ‘according to each line's ‘weigh.   19805-ERA NEURAL NETWORK DEEP LEARNING NEURAL NETWORK  Hidden Multiple hidden layers process hierarchical features,  Links cary signals from one node: +0 another, boosting ‘or damping them ‘according to each line's ‘weigh.   Classical Machine Learning  ne’ qak “a “ Drive,  Supervised Learning Unsupervised Learning (Pre Categorized Data) (Unlabeled Data )  SUPERVISED LEARNING  SUPERVISED LEARNING  KNOWN UNKNOWN KNOWN  INPUT VARIABLES, INDEPENDENT VARIABLES, FEATURES  SUPERVISED LEARNING  KNOWN UNKNOWN KNOWN  INPUT VARIABLES, (OUTPUT VARIABLES, TARGET, INDEPENDENT VARIABLES, DEPENDENT VARIABLE, LABELS FEATURES  Supervised Learning   Supervised Learning  Machine Learning Model  Training  Labeled Data  Use a linear regression model  4  Fit a line through the data  price ($)  square feet (sq.ft Be x  Supervised Learning  Macine Looming Medel Cat  Training Doo.  Labeled Data  Supervised Learning  Looming Medel th |  Training  Labeled Data HEIGHT, WEIGHT, SIZE OF THE EARS, COLOR OF THE EYES.  Unsupervised Learning Mochine ie Learning ads  cat  Dog  Unlabeled Data  Unsupervised Learning Mochine fh Learning ads  Cat  Unlabeled Data  Unsupervised Learning  Unlabeled Data  Cat   Unsupervised Learning ochine fh Learning ads f  Dog  Unlabeled Data  Unsupervised Learning Mochine fh Learning ads  ANIMAL  a  ANIMAL2  Unlabeled Data   After K-Means   Classical Machine Learning  Dats Driven  Unsupervised Learning ( Unlabeled Data )  Supervised Learning (Pre Categorized Data )  Classical Machine Learning  ned, at" ~“ Drives  Supervised Learning Unsupervised Learning (Pre Categorized Data ) ( Unlabelled Data )  “  ™  Classification ‘Regression  REGRESSION:  PREDICT A CONTINUOUS NUMERIC TARGET VARIABLE  Use a linear regression model  <  Fit a line through the data  price ($)  square feet (sq.ft. ‘Be x  Use a linear regression model  4  Fit a line through the data  price ()  square feet (sq.ft. ‘Be x  Use a linear regression model  4  Fit a line through the data  price ($)  square feet (sq.ft. ae x  CLASSIFICATION:  PREDICT DISCRETE CATEGORICAL VARIABLE (“LABEL” / “CLASS”)  KLIK  INBOX  SSE  SPAM FOLDER ie ee Ba at ee   6 E—E—Q____  FD hxt0s:17mai. google.com  Google es ~~ o  eoah Ja y= cael =z .™ 22S. ere ome . = Sa = ae eapegnyeniomepemanoera  {mtyMaion (Googe) I [aling Cub Ave we tng eight kong Aw we king nh?  ‘Sean St (Cogan) Preto tb nw pp Sean Sr shared an sou wh you. View sbum be Reh nou who ‘coos an Bayar sa pon wn you Fw a shar wi Katy tig hr a re Dont Cocaiee Danie Hoothood ade you on Google iow hae win Danie by atsng her wee Youtube Justo You From YouTube: Day Update - Jun 19,2013 Cech tn tat een far you eh ‘cooge Yu mare agen shots on Google oxy au mae ingens Eos abn es ary acoba(Googier) Checkout photon of my new apt ir nbs sae an bum wh yo View Hu eet  Googes ate Beye odtd you cn Googe - Foon and share wih Kate by dng het cr, Don hn  Mowe i —  FB hxx5:7maitgoogiecom  Google == _— i  Gma~ @ foe sean stzse | ccarcee | emry a === | == |.  omy a sep stigtn  = coe 1 Yes er gpetn 3 pete on Ong Oger wr tue tes et bun a veune 1 Lar weed es, eo te enc et  =a {tn Goo) EL ning Ew ating ght tng A) sg?  ~ a Saseamenaoe) |) rouiesns esp 0b el meron Vor ut lpn ore cone Ret ered io Plenary oy ig fr wc Otis eins oon ee tend nd oo eng wn srw Oy a rn ei = a Some a evr pet pn on rag Cage om pn reo i sane ey eb fee) en tent myo et yee ho ee you or stem a epi  = Etelomey Googe ate Beye odd you cn Googles - Foto and share wih Katey dng het cr. Dor hn  LINEAR REGRES:  INPUT, FEATURE, OUTPUT, TARGET, INDEPENDENT VARIABLE DEPENDENT VARIABLE  INPUT, FEATURE, HINEAE OUTPUT, TARGET,  INDEPENDENT VARIABLE DEPENDENT VARIABLE  INPUT, FEATURE, HINEAE OUTPUT, TARGET,  INDEPENDENT VARIABLE DEPENDENT VARIABLE  LINEAR  INPUT, FEATURE, OUTPUT, TARGET, INDEPENDENT VARIABLE DEPENDENT VARIABLE | | se=30.-57 ¥; = Bo + Ai Xi 1 = Dependent Variables (Salary)  i I = Average of Dependent Variables  Simple Linear Regression:  Salary ($)  Experience  ary  SUM ly - y')? -> min   simple Linear Regression: yb Rue vate Salary ($) > PREDICTION ERROR  sie Yi? PREDICTED VALUE  + |  + +  ms SUM ly - y')? > min  Experience   CLASSMATE’S HEIGHT AND. ‘SHOE SIZE   CLASSMATE’S HEIGHT AND SHOE SIZE   (Response Variable}  Slope Coefficient   Linear 2 Regression ae  FAVA Lx |< | |e | |e |e |e |   LOGISTIC REGRESSION  Linear Regression  Predicted Y can exceed Oand 1 range  Dependent Variable  x Independent Variable  CATEGORICAL ‘OUTPUT VARIABLE  Linear Regression  ys eceee-  Straight line  Dependent Variable  y=0! x  Predicted Y can exceed Oand 1 range  1 Independent Variable  CATEGORICAL ‘OUTPUT VARIABLE  INDEPENDENT VARIABLE  CATEGORICAL ‘OUTPUT VARIABLE  J ¢ Linear Regression  @)  Straight line Predicted Y can exceed Oand 1 range  Dependent Variabl  @  CATEGORICAL INDEPENDENT VARIABLE ‘OUTPUT VARIABLE  MALE  @)  Dependent Variabl  FEMALE  Linear Regression  Predicted Y can exceed Oand 1 range  HEIGHT/WEIGHT  Dependent Variable  Linear Regression  Predicted Y can exceed Oand 1 range  Logistic Regression  Predicted Y Lies within Oand 1 range  Dependent Variable + <  Fi Independent Variable  x Independent Variable    Logistic Regression  Predicted Y Lies within Oand 1 range  Dependent Variable + =<  < i  x Independent Variable  Logistic Regression  Predicted Y Lies within Oand 1 range  Dependent Var  < i  x Independent Variable  K NEAREST NEIGHBORS ALGORITHM (KNN)  ae es ome  % Y REGRESTION  a) CLASSIFICATION »  Dependent Variable pent Variables (Response Variable)  Yintercept Error Term   Dependent Variable Independent Variables (Response Variable) (Predictors)  \ \  Y = Bo + B1X1 + BoX2 + + €  f \ / \  Y intercept Slope Error Term Coefficient    ry  Before training  ry  ‘After training MALE MALE Nd e oe o6 ce gees Beil o% (query detapoint assigned to ‘ ‘oe Fe oO a micas 4 a ° a "ae a we. Car an, FEMALE * FEMALE" tT  Tr >  ry  Before training  ry  ‘After training MALE MALE hd e oo oe ce goes Beit oe % (query detapoint assigned to ‘ ‘oe Fe oO a micas 4 a ° a "ae a we. Cae an, FEMALE * FEMALE" tT  Tr >  CHEST CIRCUMFERENCE  3.NN for regression  HEIGHT  CHEST CIRCUMFERENCE  3.NN for regression  HEIGHT  Under-Fitting Appropriate-Fitting Over-Fitting  K=1000 K=5, K=1  Under-Fitting Appropriate-Fitting ‘Over-Fitting  00 simple to explain the variance) {force-tting- too good tobe tue  K=1000 K=5, K=1  Under-Fitting Appropriate-Fitting ‘Over-Fitting  t00 simple to explain the variance) {force-tting- too good tobe tue  K=1000 K=5 K=1  Over-Fitting  {force-tting- too good tobe tue  K=5, K=1   Under-Fitting Appropriate-Fitting  K=1000 K=5, 21    -’ CLASSIFICATION  REGRESSION  © CLASSIFICATION  © REGRESSION  DECISION BOUNDARY  DECISION BOUNDARY  Nose length  Elephants  a 4  AAA Cats a  4 a e Co @ &. Elephants (Class 1) @ Cats (Class -1)  ee  Weight  x  Nose length  Elephants  aa  Elephants (Class 1) @ Cats (Class -1)  y  Weight  x  Nose length  Elephants  aa  & Elephants (Class 1) @ Cats (Class -1)  ,  Weight  x  Elephants  a A  Nose length  A Elephants (Class 1) @ Cats (Class -1)   & Elephants margin die ok eS 5. = g& @ g 2  & Elephants (Class 1) @ Cats (Class -1)   x  Nose length  Elephants  a A  A Elephants (Class 1) @ Cats (Class -1)  BENEFITS OF SVM   HYPERPLANE  KERNEL FUNCTIONS  KERNEL FUNCTIONS  KERNEL FUNCTIONS  NON LINEAR DECISION BOUNDARY  IMPLICIT FEATURE ENGINEERING  Ly»)  Kernel Optimization No . Formula function parameter Dot @,,0;) = (x,,2, : ; -product Kener) = (mes) c K(a,,0;) = 2 | RBF 2 C and y exp(=7| a ~#; |? + 5 | siemoia | Atm = c 4 yemor tanh (7(2,,2;) +r) i Saini Poly- K(a,,;) = 4 . d Coynd nomial (y(w..0,) +7)  Kernel Optimization No . Formula function parameter Dot @,,0;) = (x,,,2, : ; -product Klar) = (mes) c K(a,,0;) = 2 | RBF 2 C and y exp(=7| a, ~2; |? + 5 | siemoia | Atm = c 4 semor tanh (7(2,,2;) +r) i Saini Poly- K(a,,2;) = 4 . d Coynd nomial (v(x...) +r)  \  Probability of B occuring  P(B|A) - P(A) 2@|B) =— a / Pe)  %  Probability of B occuring  Label sms  (0 spam SECRET PRIZE! CLAIM SECRET PRIZE NOW!  1 ham ‘Coming to my secret party?  2° spam Winner! Claim secret prize now! Label secret prize claim now coming to my party winner sem 202 ooo 0 oO ham 1 000 (0 0 Tdi i 0 sem 1 1 td 000 0 1  Label sms  (0 spam SECRET PRIZE! CLAIM SECRET PRIZE NOW!  1 ham ‘Coming to my secret party?  2° spam Winner! Claim secret prize now! Label secret prize claim now coming to my party winner sem 202 ooo 0 oO ham 1 000 (0 0 Tdi i 0 sem 1 1 td 000 0 1  P(span| penis, viagra )  _ P(penis|spam) * P ( viagra| spam)» P (spam) P( penis) P( viagra)  24, 20, 30, = 30 30 74 25. 51  7a" 7]  =0.928 92.8% SPAM  P(spam| penis, viagra )  _ P(penis|spam) * P ( viagra| spam)» P (spam) P( penis)» P( viagra)  24, 20, 30, = 30 30 74 25. 51  7a" a7  =0.928 92.8% SPAM  P(viagra | spam) = P(viagra | spam & penis)  PROBABILITY OF THE WORD “VIAGRA” APPEARING IS INDEPENDENT OF THE WORD “PENIS” APPEARING (FEATURE INDEPENDENCE)   USE CASES FOR NAIVE BAYES:  SPAM CLASSIFICATION OTHER TEXT BASED CLASSIFICATION  DECISION TREE ANIMAL CLASSIFIER  Nigh bea  orf ight gate sri) Might rat  Onto Inwater  ight bean lphane  Wigit be a  Might artincars MOO  DECISION TREE ANIMAL CLASSIFIER  Rot  Comet squat ~| tan sat Lang  Nigh bea  ight ea grate squitel Wight be rat  Ontand Inwater  ight bean logan  ight bes  Mihtbe artis Me  DECISION TREE ANIMAL CLASSIFIER  Rot  Comet squat ~| tan squat Lang  Nigh bea  ight ea grate squitel Wight be rat  Ontand Inwater  ight bean logan  ight bes  Mihtbe artis Me  Understanding the risks to prevent a heart attack.  Sib Tree  Understanding the risks to prevent a heart attack.   Split A Split B Low(er) Purity High(er) Purity   Split A Split B Low(er) Purity High(er) Purity   Split A Split B Low(er) Purity High(er) Purity  True // L \raise True/ ef \ zalse  [e*e][o*a] [Poe] a *al  @2/3 42/3 @3/3 A3/3   BAGGING   Original Data  C0088 CCCOCOCO 5 : * Bootstrap sample 1 _Bootstrap sample 2 Bootstrap sample n eooe| (coecce|  [cccce   Instance  Class-8   Beciaiea hee Random Foreat Left a iM Right Left i Node Node Node Node Node Node  "Node splitting in a random forest models based on a random subset of features foreach tree.  learning & regularization  Balanced  Underfitting   BOOSTI  Bagging and Boosting  Bagging Boosting ®) ¥) %) =) @» 6» Parallel Sequential  rrwedc.com,  Bagging and Boosting  Bagging Boosting x) ®) ~@) =) @- 6» Parallel Sequential  rrwceducs.com,  Weak leamer1 _Weakleamer2__ Weak learner 3  “Sirong learner  KICK Xx xX  Under-fitting Appropirate-fitting Over-fitting  (too simple to (forcefitting-too explain the variance) good to be true) DG,   Under-fitting Appropirate-fitting Over-fitting (200 simple to (forcefitting-too explain the variance) good to betrue) DG  SLOWER TO TRAIN THAN RANDOM FORESTS  ADABOOST GRADIENT BOOSTING XGBOOST  NEURAL NETWORKS & DEEP LEARNING  Deep neural network   Multiclass Classification  x  i i  “A  { i  fe)  i i  ry  { i  I   OCC DDDOHOOSIO0000  WMH BH Kd NMKRYOROe NMYSY RES NHROVRAT Norhs hoa NEOPSV ERO RoeThs Kwv NO XYD do > IMHIAYD KR AMTEDRAHOD NPALSRAS Nm rawrwn NMFS YD hres AMBLOKR®N HM TP HS RB &   C0 D0D0DO0HOCOS6IO0000  WMH BH KRV®d NMKRYDOROe NMYSY RES NHROV RAT Norhs Poa NEOPSV ERO HoeThs Kw NO RVD dod AMITAY KOO AMTYDR YS NPALSHROS Nira we ron NMFS YD hes AMBLO KAN HM THs RR &  C0 0D0D0HOCO6O00000  he KD YOnan YU Reo OV RAST He POR \Y EAD bh Kw H YW do & AN Ke ¥ORSOD LO hdsn ee QO fh mwD Yon gan WS RY  33  3  2222272822322 2229 i) 3  33  CSC D0DDDOHOCOS6IO0000  2228223222229 * 3 32333  ad 333  bon yen  oo yeh ye hs ys ADK ve 4s Wer \v rt Yon vs &  Soo 2727 FNGLA 7 2  C0 D0D0DOHOCOSIO0000  A2Xg22292232422Z229  32333  3  a  33  aN yen  no beh ye hs ys AN Kw veo  4s  \v rt Yon ns &  S59 27 ENGI SLAG 7 2  ~C Sq 8   Poly SVM_ sigmoid SVM  Linear svM BF svi o | i Unearsvm REF SVM  Linear SVM  97   HOW A DEEP NEURAL NETWORK SEES   '905-ERA NEURAL NETWORK DEEP LEARNING NEURAL NETWORK   (2) (#) on  Input Layer Output Layer  output layer input layer  input layer  Simple rules can’t do it  (J   Simple rules can’t do it  a   Simple rules can’t do it  agonal   Simple rules can’t do it  agonal   Simple rules can’t do it  agonal   Deep era eter   Deep era eter   Deep Nera eter   beep Neral eter   ect mals  ‘onbinvtens of edpes  Obj:  ical Machine Learning 0°? Drive,  Unsupervised Learning ( Unlabelled Data)  “  Ditnensionatiey  Supervised Learning (Pre Categorized Data)  Clustering sociation  (Divide by Cleencey gy Simitariey) Sueno (Wider Ee. Identiey Eg. Market Eg. Targeted Eg. Customer eee Fraud Detection Forecasting ‘Marketing Recommendation _‘5-Bis Daca Visualization Predications 4 Predictive Models Pattern/ Structure Recognition  KNOWN UNKNOWN KNOWN    CLUSTERING  CLASSIFICATION CLUSTERING  Supervised Learning Unsupervised Learning (a) (b)  CLASSIFICATION CLUSTERING  Supervised Learning Unsupervised Learning (a) (b)  CLASSES KNOWN  CLASSIFICATION CLUSTERING  Supervised Learning Unsupervised Learning (a) (b)  CLASSES KNOWN CLASSES UNKNOWN  CLASSIFICATION CLUSTERING  Supervised Learning Unsupervised Learning (a) (b)  CLASSES KNOWN CLASSES UNKNOWN   CLUSTERED DATA VISUALIZATION  RAW DATA   CLUSTERED DATA VISUALIZATION  RAW DATA   K IS AHYPERPARAMETER    | eo step3 oy . 7° Stepa * .  - © Step3   . * Step1 Step2 4 te kt” steps * . . - o step3   and ros ° * Step 1 Step2 1   : wes. “eS steps * . . Step1 Step2 4 . 2 #8 step ot . : o* steps   : oe oe “eS steps * . a Step 1 Step 2 4 . 2 8 stepa   | eo Step3 oa . TS Steps o* .  - © Step3   : wes. “ef 6" steps ° . * Step1 Step2 4 . 2 stepa ot . : 2° steps     Dimensionality  yf = > |i | x  Dimensionality  7 ff = > |i |  fae mente -—— tt   Fig. 5. Example of two Airbus airplane images (top A380, sottom A330-200) at resolutions 200 x 200 (left), 50 x 50 ‘middle) and 20 x20 (right). Resolution degradation occludes  he discriminating details that enable distinguishing classes.   cet  Fig. 5. Example of two Airbus airplane images (top A380, vottom A330-200) at resolutions 200 x 200 (lefi), 50 x 50 ‘middle) and 20°20 (right). Resolution degradation occludes he discriminating details that enable distinguis   Fig. 5. Example of two Airbus airplane images (top A380, sottom A330-200) at resolutions 200 x 200 (left), 50 x 50 ‘middle) and 20°20 (right), Resolution degradation occludes  he discriminating details that enable distinguishing classes.   Data Preprocessing  [ | |  Data Data Data Cleaning Transformation Integration  Data Reduction  Removing + Sealing + Joining Duplicates + Encoding + Merging Handling  missing values  PRINCIPAL COMPONENT ANALYSIS (PCA)           Eigenvalue  ° Oo  Fl F2 F3 F4 FS FO F7 FB FO FLOFILFI2 FIZ FI4 FIS  Factorial axis (PC)  100  Cumulative variance (%)  Eigenvalue  : Hnletataterer  Fl F2 F3 F4 FS FO F7 F8 FO FIOFILFI2 FI  Factorial axis (PC)  q  100  20  Cumulative variance (%)  scikitlearn algorithm cheat-sheet   scikitlearn algorithm cheat-sheet